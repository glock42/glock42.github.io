<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 一派胡言</title><meta name="description" content="我向往安静且自由的生活"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://threezj.com/atom.xml" title="一派胡言"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://github.com/zyycj" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/10/27/wisckey/" class="post-title-link">论文笔记 [FAST '16] WiscKey, Separating Keys from Values in SSD-conscious Storage</a></h2><div class="post-info">2018年10月27日</div><div class="post-content"><p>LSM-Tree的优势在于借鉴LFS的思想，将大块的内存连续的写出到磁盘，减少磁盘seek的时间。同时输出的格式又是连续的，查找的速度也比较快。但是LSM-Tree的结构，也带来了写放大和读放大的问题。这些影响在hdd上是值得的，但是在ssd上并不友好。WiscKey提出了一种面向ssd的，将key和value分开存储的方法。</p>
<h2 id="Write-and-Read-Amplification"><a href="#Write-and-Read-Amplification" class="headerlink" title="Write and Read Amplification"></a>Write and Read Amplification</h2><p>写放大主要在于compaction的，每次两层之间做compact时，都需要将多个sstable读出做排序（读放大），再写到磁盘。而读放大方面，LSM-Tree需要查找多个数据结构，memtable-&gt;imutable-&gt;level files，假设不存在这个key的，便要把每一层做二分查找搜一遍。特别是第0层，有overlap，需要每个文件都看，虽然有bloom filter，但也影响效率。当value比较大的时候，问题就很明显了，compaction时sstable的读入和写出，都是将key和value一起读一起写的，当value变长时，效率会很低。这些放大的影响在hdd，来换取磁盘seek的消耗还是值得，但是在SSD上就不一样了，SSD随机读写要快的多，并且有并行随机读取的特性可以利用。</p>
<p><img src="https://zhang.nos-eastchina1.126.net/blog/WX20181027-212041%402x.png" alt=""><br></div><a href="/2018/10/27/wisckey/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/10/14/clock/" class="post-title-link">论文笔记 Time, Clocks and the Ordering of Events in a Distributed System</a></h2><div class="post-info">2018年10月14日</div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在分布式系统中，如何确定两个事件发生的先后顺序是比较困难的。在不同机器上的物理时钟会有会有误差。Lamport在1978的文章<a href="http://research.microsoft.com/users/lamport/pubs/time-clocks.pdf" target="_blank" rel="noopener">Time, Clocks and the Ordering of Events in a Distributed System” (1978)</a>提出了一种Logical Clock 来描述分布式系统中的先后顺序。论文中先是定义了一种偏序的<code>happened before</code> 关系，通过这种关系给出了logical clock的算法，最后用logical clock实现了全序的分布式算法。</p>
<h2 id="Happened-before"><a href="#Happened-before" class="headerlink" title="Happened-before"></a>Happened-before</h2><p>Lamport提出用事件发生的先后因果关系来描述事件。若事件b依赖于a发生，则<code>a happened-before b</code>。定义如下。</p>
<ol>
<li>If a and b are events in the same process, and a comes before b, then a → b.</li>
<li>If a is the sending of a message by one process and b is the receipt of the same message by another process, then a → b.</li>
<li>if a→ b and b → c then a→c. </li>
</ol>
<p>若相互之间没有依赖关系，可认为是并行的。并发说明这两个事件谁先发生并不重要，这其实逻辑时钟的重要思想。Logical clock只保证你的系统不出错（即不违反相互依赖关系），而不能保证事件发生的真实顺序。<br></div><a href="/2018/10/14/clock/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/09/26/paxos/" class="post-title-link">论文笔记 Paxos made simple</a></h2><div class="post-info">2018年9月26日</div><div class="post-content"><h2 id="Basic-Paxos"><a href="#Basic-Paxos" class="headerlink" title="Basic Paxos"></a>Basic Paxos</h2><blockquote>
<p>The Paxos algorithm, when presented in plain English, is very simple. </p>
</blockquote>
<p>Paxos有许多变种，一般来说都是指的Basic Paxos，也就是这篇论文里提出来的内容。下面没有特殊说明，都是指的Basic Paxos。</p>
<p>Paxos需要保证的是两个特性。</p>
<ol>
<li><p>safety</p>
<ul>
<li>Only a value that has been proposed may be chosen</li>
<li>Only a single value is chosen, and</li>
<li>A process never learns that a value has been chosen unless it actually has been.</li>
</ul>
</li>
<li><p>liveness</p>
<ul>
<li>Some proposed value is eventually chosen</li>
<li>If a value is chosen, servers eventually learn about it</li>
</ul>
</li>
</ol>
<p>Paxos和raft一样也划分了三种角色，但是和raft的不同的是一台机器可以同时扮演这三个角色。</p>
<ul>
<li><p>Proposer</p>
</li>
<li><p>Acceptor</p>
</li>
<li><p>Learner</p>
</li>
</ul></div><a href="/2018/09/26/paxos/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/08/20/NFS vs AFS/" class="post-title-link">NFS vs AFS</a></h2><div class="post-info">2018年8月20日</div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>周末读了ostep的两篇文章<a href="http://pages.cs.wisc.edu/~remzi/OSTEP/dist-nfs.pdf" target="_blank" rel="noopener">Sun’s Network File System</a>和<a href="http://pages.cs.wisc.edu/~remzi/OSTEP/dist-afs.pdf" target="_blank" rel="noopener">The Andrew File System</a>，都是讲的分布式文件系统，但是侧重的方向不同，导致相关实现也全然不同，写篇笔记对比一下。</p>
<h2 id="NFS"><a href="#NFS" class="headerlink" title="NFS"></a>NFS</h2><p>NFS的goal是fast crash recovery and simple，所以它的设计都是为这个目的服务。</p>
<h4 id="key-design"><a href="#key-design" class="headerlink" title="key design"></a>key design</h4><ul>
<li><p>stateless  </p>
<p>server不保存任何有关client的状态。假如crash，不用做任何操作，直接重启即可。这是一个最重要的设计。需要的信息都通过rpc的参数传递过来。</p>
</li>
<li><p>idempotent</p>
<p>接口都做到幂等性，这样做主要是为了处理message lost 或者 server crash这些情况，client只需要retry即可。</p>
</li>
<li><p>client-cache</p>
<p>这主要为性能考虑，但同时为带来一致性问题。NFS通过前先通过getattr request发送给server，查看cache是否过期，一般是周期性的问一下，比如所3s。同时在close的时候将cache刷回server。</p></div><a href="/2018/08/20/NFS vs AFS/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/07/05/Database concurrency control note/" class="post-title-link">Database concurrency control note</a></h2><div class="post-info">2018年7月5日</div><div class="post-content"><h2 id="Locking-in-B-Tree"><a href="#Locking-in-B-Tree" class="headerlink" title="Locking in B+Tree"></a>Locking in B+Tree</h2><p>使用2PL在index上效果会很差，因为每次使用索引时都会lock root，导致其他事务无法访问。Index一般使用<code>Lock crabbing</code>。</p>
<h4 id="Basic-Lock-Crabbing"><a href="#Basic-Lock-Crabbing" class="headerlink" title="Basic Lock Crabbing"></a>Basic Lock Crabbing</h4><ol>
<li>search<ul>
<li>获取parent的S lock</li>
<li>接着到下一层获取child的S lock</li>
<li>释放上一层parent的S lock，如此循环。</li>
</ul>
</li>
<li>insert/delete<ul>
<li>获取parent的X lock</li>
<li>到下一层，获取child的X lock</li>
<li>如果安全的话则释放parent的X lock。安全即是指child没有分裂或者合并。也就是说有足够的空间插入，或者足够多的节点删除。不然继续到一层，如此循环。</li>
</ul>
</li>
</ol>
<p>在删除或者插入的情况下，如果节点都满或者都不够的话很有可能整条链上都有锁，一直到leaf节点才会逐级向上释放，并发性比较差，由此引入一种优化的方案。<br></div><a href="/2018/07/05/Database concurrency control note/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/06/20/Database join algorithm note/" class="post-title-link">Database join algorithm note</a></h2><div class="post-info">2018年6月20日</div><div class="post-content"><h2 id="Join-algorithm"><a href="#Join-algorithm" class="headerlink" title="Join algorithm"></a>Join algorithm</h2><ul>
<li>Simple Nested Loop Join</li>
<li>Block Nested Loop Join</li>
<li>Index Nested Loop Join</li>
<li>Sort-Merge Join</li>
<li>Hash Join</li>
</ul>
<p>下面以这两张表为例</p>
<p><img src="http://zhang.nos-eastchina1.126.net/blog/WX20180620-212956.png" alt=""></p>
<h2 id="Simple-Nested-Loop-Join"><a href="#Simple-Nested-Loop-Join" class="headerlink" title="Simple Nested Loop Join"></a>Simple Nested Loop Join</h2><p>即简单的双重循环。对每一个外层table中的tuple都要scan一遍内层table</p>
<p><code>Cost: M + (m*N)</code><br></div><a href="/2018/06/20/Database join algorithm note/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/05/23/Database Storage and Buffer pools note/" class="post-title-link">Database Storage and Buffer pools note</a></h2><div class="post-info">2018年5月23日</div><div class="post-content"><h2 id="Goals-of-the-DBMS"><a href="#Goals-of-the-DBMS" class="headerlink" title="Goals of the DBMS"></a>Goals of the DBMS</h2><ul>
<li>Allow the DBMS to manage databases that exceed the amount of memory available</li>
<li>Reading/writing to disk is expensive, so it must be managed carefully</li>
</ul>
<p>DBMS总是希望自己来管理所有东西，而不是依靠操作系统</p>
<h2 id="File-Storage"><a href="#File-Storage" class="headerlink" title="File Storage"></a>File Storage</h2><ul>
<li>最简单的形式，一张表存储一个文件。但是也有多个关联的表存在一个文件的实现。</li>
<li>操作系统对于db的文件内容是不关心。</li>
</ul>
<p>每个file由多个page组成，有多种不同的方式来存储，</p>
<ul>
<li>Heap File</li>
<li>Sequential File</li>
<li>Hashing File</li>
<li>Log-Structured File</div><a href="/2018/05/23/Database Storage and Buffer pools note/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/04/28/LFS/" class="post-title-link">论文笔记 [SOSP '91] The Design and Implementation of a Log-Structured File System</a></h2><div class="post-info">2018年4月28日</div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这两天花了点时间读了下<code>The Design and Implementation of a Log-Structured File System</code>，92年的一篇经典文章。Idea其实很简单，就是利用大内存缓存足够多的内容后，然后一次性顺序的写到磁盘中。LFS的所有写入都是顺序添加。极大提升写效率。思想简单，但是实现比较tricky，有各种细节需要注意，却不复杂，很符合人类直觉。</p>
<h2 id="位置在变化的inode"><a href="#位置在变化的inode" class="headerlink" title="位置在变化的inode"></a>位置在变化的inode</h2><p>LFS比较关键的一个点，在于inode不是存在一个fix的位置，而是每次写入一个新的块的时候，都会生成新的inode。之所以这样做是因为，传统的文件系统将inode放在fix位置，当添加或者更新块的时候，需要seek到多个位置去查找inode，查找间接块等，物理运动极其耗时。而LFS这样做了之后，只需要顺序接下去写就行了，但如果是添加的话，还需要读取一次旧的inode块，来维护新的inode块。</p>
<p>但是寻找inode又是一个问题，传统文件中，反正inode数组位置是固定的，只需要位置加上filenumber偏移量就可以了。而在LFS中，存在一个叫imap的数组来索引到最新的inode。也就是说当每次写的时候，在最后总会加上一块imap，来索引最新的inode地址（旧的失效）。到最后整个磁盘中有很多分块的imap。</p>
<p>但是最后还是需要一块fix的位置来存磁盘中所有最新的imap的地址，即checkpoint region(CR)。比如说放在磁盘最开始的地方。CR肯定是需要磁盘seek过去更新，不过这个更新是周期性的，长时间的，比如30秒。而且所有最新imap，是足够小，可以存在内存的。所以只需要磁盘挂载的时候一次性seek到各个位置读入一次所有的imap即可（所有目的都是为了减少seek）。</p>
<p><img src="http://zhang.nos-eastchina1.126.net/blog/WX20180428-174248@2x.png" alt=""></p>
<p>大概情况如上图。当然还有目录的情况，目录其实也是个文件(保存file name: file number的映射)    ，所以处理其实是和普通文件一样的。<br></div><a href="/2018/04/28/LFS/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/06/11/Raft 共识算法/" class="post-title-link">Raft 共识算法</a></h2><div class="post-info">2017年6月11日</div><div class="post-content"><h2 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h2><p>关于<code>mit 6.824</code>，这门课在找工作期间，又拾起来继续做了，断断续续的把<code>lab2</code>做完了，pass掉所有test。完整的实现了<code>Raft</code>算法，对分布式的了解也算是更进了一步。此文会对<code>Raft</code>做一个归纳总结，大部分的内容基于<a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf" target="_blank" rel="noopener">raft 论文</a> 。</p>
<p><code>Raft</code>是一个共识算法，目的是为了保证多个副本在多个服务器的情况下数据的一致性，以及保证一定的容错能力。</p>
<p><code>Raft</code>的卖点是<code>understandable</code>，论文中一直在强调可理解（这个可理解对没接触过分布式的人来说，其实也不太好理解…）。这个可理解应该是相对于<code>paxos</code>来说的。基于<code>understandable</code>这个目标，<code>Raft</code>把整个共识的过程分成三块，分别是<code>leader election</code>， <code>log replication</code>和<code>safety</code>，下面我们会一一介绍。<br></div><a href="/2017/06/11/Raft 共识算法/" class="read-more">...阅读全文</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2016/12/30/数据库事务与并发控制/" class="post-title-link">数据库事务与并发控制</a></h2><div class="post-info">2016年12月30日</div><div class="post-content"><h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><blockquote>
<p>数据库事务是<code>DBMS</code>执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。</p>
</blockquote>
<p>事务必须要满足<code>ACID</code>四个特性。</p>
<ul>
<li><p><code>Atomicity</code>(原子性)</p>
<p>也就是所谓的<code>all-or-nothing</code>。单个事务有一系列的操作，比如查找，增加，更新等。当事务提交之后事务中所有对数据库的操作必须反映到数据库上。但如果由于某些原因中断，那么所有对数据的操作必须恢复到这个事务开始之前。也就是事务对数据库的操作，要么全部执行，要么都不执行。</p>
</li>
<li><p><code>Consistency</code>(一致性)</p>
<p>事务开始之前数据满足的完整性约束，在事务结束之后也需要满足。其实也就是事务得到了正确的执行。</p>
</li>
<li><p><code>Isolation</code>(隔离性)</p>
<p>每个事务之间是相互独立的。有点类似于进程的概念。因为事务是并发执行的，所以事务之间不能相互影响不然就有可能破坏<code>一致性</code>。</p>
</li>
<li><p><code>Durability</code>(持久性)</p>
<p>已被提交的事务对数据库的修改应该永久保存在数据库中。这里的关键应该在于永久二字，也就是说即使系统崩溃、硬盘损坏等不可控因素发生，也要保证数据仍然存在数据库中。所以备份很重要啊。</p></div><a href="/2016/12/30/数据库事务与并发控制/" class="read-more">...阅读全文</a></article></li></ul></main><footer><div class="paginator"><a href="/page/2/" class="next">下一页</a></div><div class="copyright"><p>© 2015 - 2018 <a href="http://threezj.com">Jian Zhang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>
<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 为什么机器学习真的可以学到东西？ · 一派胡言</title><meta name="description" content="为什么机器学习真的可以学到东西？ - Jian Zhang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://threezj.com/atom.xml" title="一派胡言"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://github.com/zyycj" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://weibo.com/zjthree" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">为什么机器学习真的可以学到东西？</h1><div class="post-info">2016年7月27日</div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>开始跟《机器学习基石》这门课，相对于<code>Stanford</code>那门课，这门明显难度大很多，我跟到第10个<code>Lecture</code>，才刚刚讲到<code>Logistic Regression</code>。前面费了很大力气在讲机器什么时候可以学习，以及证明为什么能学习。</p>
<p>此文主要是基于《机器学习基石》的学习笔记。<code>Topic</code>是为什么机器可以学习？</p>
<h2 id="机器学习流程"><a href="#机器学习流程" class="headerlink" title="机器学习流程"></a>机器学习流程</h2><p>下面是一个粗略的机器学习流程图</p>
<p><img src="http://beader.me/mlnotebook/section2/images/basic_setup_of_the%20_learning_problem.png" alt=""></p>
<p>机器学习最开始也是最终的目的是获得一个<code>target function</code>，喂进去数据能直接得到正确结论的函数。为了得到这个函数，我们需要一大堆的训练数据。然后通过一个好的机器学习算法，从一大堆<code>可能的function(也就是H)</code>中挑选一个<code>比较好的function(也就是g)</code>，这个<code>g</code>和<code>target function</code>长得越像越好。<br><a id="more"></a></p>
<h2 id="Hoeffding’s-inequality"><a href="#Hoeffding’s-inequality" class="headerlink" title="Hoeffding’s inequality"></a>Hoeffding’s inequality</h2><p>大家有没有想过，为什么这样就能学到东西。我们的算法只是在训练数据上跑，从训练数据跑出来的<code>g</code>，我们怎么能确定它也能在测试数据上跑的很好呢？这个就是问题的关键。其实接下来内容主要就是论证这个问题。</p>
<p>先来考虑一个简单的问题。比如说我们现在有一个黑罐子，里面有很多弹珠，只有两种颜色，黄的和绿的。好现在问你，你怎么能知道黄色弹珠大概有多少颗？</p>
<p><img src="http://beader.me/mlnotebook/section2/images/bin_sample.png" alt=""></p>
<p>大家肯定都会说抽样。没错，我们抽出10个弹珠，很容易能知道黄色弹珠在<code>sample</code>中的比例。但是这个比例真的能代表罐子中的比例吗？也许能，也许不能。而且能的记录会随着我们<code>sample</code>数目的增大而增大。但是也有可能你抓出一把全绿。但这种情况发生的记录很小。这里我们有一个定理保证这种偏差发生的记录很小。</p>
<p><code>Hoeffding&#39;s inequality</code>可以保证偏差很大发生的几率很小，并且随着<code>N</code>的增大很减小。公式如下，<code>v</code>代表<code>sample</code>中黄色弹珠的比例，<code>μ</code>表示罐子中黄色弹珠的比例。<code>ϵ</code>也就是偏差。</p>
<blockquote>
<p>ℙ[|ν−μ|&gt;ϵ]≤2exp(−2(ϵ^2)N)</p>
</blockquote>
<h2 id="坏事的发生"><a href="#坏事的发生" class="headerlink" title="坏事的发生"></a>坏事的发生</h2><p>现在我们称<code>v</code>为<code>Ein</code>,<code>μ</code>为<code>Eout</code>，现在我们已经证明了<code>Ein</code>和<code>Eout</code>不会差的太远，更重要的事情是保重<code>Ein</code>越小越好，这就需要一个好的算法。<br>还记得上面的学习流程吗，我们的算法是从很多个<code>h</code>中去挑选一个<code>Ein</code>最小的<code>h</code>让它成为<code>g</code>。但是这里会有坏事情发生。<br>所谓的坏事情就是<code>bad sample</code>，就是说我们抽出了十个全是绿的弹珠。现在有一个好的<code>h</code>称之为<code>h1</code>，和坏的<code>h</code>叫<code>h2</code>，<code>h1</code>对于这个<code>bad sample</code>的表现当然是糟糕的，而恰好<code>h2</code>表现很好，那<code>h2</code>就被选成<code>g</code>了。</p>
<p>当出现坏事的时候，我们学习就会困难，可以直接说不能学习。所以这个坏事出现的概率是多少呢？把所有h中发生坏事的几率加起来。</p>
<p><img src="http://7xrsib.com1.z0.glb.clouddn.com/Screenshot.png" alt=""></p>
<p>从上图的式子中可以看到，坏事发生的几率和<code>M</code>有关。<code>M</code>也就是<code>h</code>的个数。<br>从现在的条件来看，如果<code>M</code>很大甚至无线的话那么<code>Learning</code>是不可行的。</p>
<h3 id="无效的Hypothesis"><a href="#无效的Hypothesis" class="headerlink" title="无效的Hypothesis"></a>无效的Hypothesis</h3><p>真实的情况是<code>M</code>一般不会很大，请再仔细看看上一张图的推导，<code>M</code>是通过把所有的<code>h</code>坏事发生的概率加起来的，但是其实这些<code>h</code>不是互相独立的。所以这些<code>h</code>是有重复的，如下图。</p>
<p><img src="http://beader.me/mlnotebook/section2/images/hypothesis_overlap.png" alt=""></p>
<p>比如说，我们想学习的<code>target function</code>是一条把<code>x1</code>分类成正负的线。现在<code>h</code>就有无数个，因为任意一条线都能分类，但是实际有意义的只有两种，分成正的和负的。<br>如果是两个点的话，实际有效的<code>h</code>就有4种，但是3个点就有可能不到8种了，因为会出现三点共线的情况。4个点的话按理说有16种，但是同样有一种情况不会发生，请看下图。</p>
<p><img src="http://7xrsib.com1.z0.glb.clouddn.com/Screenshot1.png" alt=""></p>
<p>所以现在我们的公式就变成了这样，大大减小<code>M</code>的个数</p>
<p><img src="http://7xrsib.com1.z0.glb.clouddn.com/Screenshot22.png" alt=""></p>
<h2 id="成长函数的上限"><a href="#成长函数的上限" class="headerlink" title="成长函数的上限"></a>成长函数的上限</h2><p>现在我们给上面<code>effective(N)</code>一个称呼，叫做成长函数。也就是说，对于某一个输入<code>D</code>，<code>H</code>最多能够产生的多少种方程。注意是种类的数量。<br>这个所谓的种类我们也给一个定义叫做<code>dichotomy</code>,用来表示<code>H</code>对与<code>D</code>的二元分类情况。<br>好，现在问题的关键，就是<code>H</code>到底能把<code>D</code>分成多少个<code>dichotomy</code>。也就是它的成长函数到底是多少？<br>但是我们很难确定它的成长函数。但是好在我们拥有一个叫做<code>break point</code>的东西，这就是成长函数的上限。我们再看回上面分类的例子。</p>
<ol>
<li>一个点能分成两种</li>
<li>两个点分成四种</li>
<li>三个点分成六种或者八种</li>
<li>四个点只有14种(<code>break point</code>)</li>
</ol>
<p>这里的输入为三个点就是一个<code>break point</code>。也就是说当输入N个点，<code>H</code>不能够把这个<code>N</code>个点的排列组合全部表示出来时<code>(2^N)</code>，<code>N</code>就是一个<code>break point</code>。<br>当<code>H</code>能把<code>N</code>的全部组合表示出来时，说明这<code>N</code>个点被<code>H</code>给<code>shatter</code>掉了</p>
<p>我们用<code>B(N,k)</code>来表示当输入<code>N</code>个点时，<code>H</code>可以最多产生多少个<code>dichotomy</code>。<br>通过数学归纳法我们可以证明到</p>
<p><img src="http://7xrsib.com1.z0.glb.clouddn.com/ssss.png" alt=""></p>
<h2 id="VC-BOUND"><a href="#VC-BOUND" class="headerlink" title="VC BOUND"></a>VC BOUND</h2><p>现在到了最后一步，除了把上边那个成长函数的上限代入进去之外，还需要进行一系列的变形，这些变形需要很强的数学能力和概率上面的知识，我自己都不太懂，况且我觉得大部分人都不需要了解。这里我就略过，有兴趣的强人自己<code>google</code>咯。<br>最终的式子如下</p>
<p><img src="http://7xrsib.com1.z0.glb.clouddn.com/okkkkkkk.png" alt=""></p>
<p>好了，现在我们终于能说机器学习确实可以学到东西了。但是需要满足三个条件。</p>
<ol>
<li>有一个好的<code>H</code>(拥有<code>break point</code>)</li>
<li>足够多的数据</li>
<li>好的算法，能够使<code>Ein</code>足够小</li>
</ol>
<p>这三者的关系如下图。</p>
<p><img src="http://7xrsib.com1.z0.glb.clouddn.com/vvvvvv.png" alt=""></p>
<p><code>dvc = k - 1</code>，大致上可以把它看出<code>theta</code>的维度加1</p>
<p>上图很清晰的说明，并不是说你的模型搞得很复杂，算法弄得很好，就能学好，反而是取到一个折中的点，这样的学习才最有效。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2016/07/28/进程与线程/" class="prev">上一篇</a><a href="/2016/06/25/Tomcat 架构探索/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'seansun';
var disqus_identifier = '2016/07/27/为什么机器可以学习？/';
var disqus_title = '为什么机器学习真的可以学到东西？';
var disqus_url = 'http://threezj.com/2016/07/27/为什么机器可以学习？/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//seansun.disqus.com/count.js" async></script><div class="copyright"><p>© 2015 - 2018 <a href="http://threezj.com">Jian Zhang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>